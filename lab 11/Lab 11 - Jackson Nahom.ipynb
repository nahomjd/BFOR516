{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06fee567",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.sentiment import vader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b626310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Unnamed: 0         PRES  TURN_AT_TALK          CEO    WORDCOUNT  \\\n",
      "count  1114.000000  1114.000000   1114.000000  1114.000000  1114.000000   \n",
      "mean    556.500000     0.581688     15.447935     0.793537    18.205566   \n",
      "std     321.728405     0.493504     19.025057     0.404949     8.940149   \n",
      "min       0.000000     0.000000      2.000000     0.000000     1.000000   \n",
      "25%     278.250000     0.000000      3.000000     1.000000    12.000000   \n",
      "50%     556.500000     1.000000      4.000000     1.000000    17.000000   \n",
      "75%     834.750000     1.000000     24.000000     1.000000    23.000000   \n",
      "max    1113.000000     1.000000     93.000000     1.000000    62.000000   \n",
      "\n",
      "       Restatement Topic        FRAUD  \n",
      "count        1114.000000  1114.000000  \n",
      "mean            0.176840     0.352783  \n",
      "std             0.381705     0.636108  \n",
      "min             0.000000     0.000000  \n",
      "25%             0.000000     0.000000  \n",
      "50%             0.000000     0.000000  \n",
      "75%             0.000000     1.000000  \n",
      "max             1.000000     2.000000  \n"
     ]
    }
   ],
   "source": [
    "call_data = pd.read_csv('data/earningscall_fraud.csv')\n",
    "\n",
    "print(call_data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19b6a394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17684021543985637\n"
     ]
    }
   ],
   "source": [
    "# percent of fraud\n",
    "print(call_data['Restatement Topic'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "795621f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence      Welcome to Northwest Pipe's conference call an...\n",
      "clean_text    [welcom, northwest, pipe, confer, announc, ear...\n",
      "Name: 1, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from gensim.parsing.preprocessing import preprocess_string\n",
    "from gensim import corpora\n",
    "\n",
    "call_data['clean_text'] = call_data['Sentence'].apply(preprocess_string)\n",
    "print(call_data.loc[1, ['Sentence', 'clean_text']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc2ae3ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(1116 unique tokens: ['diann', 'thank', 'announc', 'confer', 'earn']...)\n"
     ]
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary(call_data['clean_text'])\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e72de9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus = [dictionary.doc2bow(text) for text in call_data['clean_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91b09501",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "\n",
    "lda_10 = models.LdaModel(bow_corpus, num_topics=10, id2word=dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a27c0a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0 : 0.031*\"quarter\" + 0.026*\"think\" + 0.020*\"certainli\" + 0.020*\"bit\" + 0.019*\"littl\" + 0.017*\"price\" + 0.015*\"ye\" + 0.010*\"margin\" + 0.009*\"brian\" + 0.009*\"product\"\n",
      "Topic 1 : 0.029*\"million\" + 0.025*\"cost\" + 0.022*\"quarter\" + 0.021*\"project\" + 0.018*\"water\" + 0.017*\"steel\" + 0.016*\"year\" + 0.013*\"fourth\" + 0.011*\"transmiss\" + 0.011*\"think\"\n",
      "Topic 2 : 0.035*\"million\" + 0.024*\"project\" + 0.023*\"quarter\" + 0.019*\"expect\" + 0.018*\"revenu\" + 0.017*\"water\" + 0.016*\"steel\" + 0.014*\"group\" + 0.012*\"product\" + 0.012*\"year\"\n",
      "Topic 3 : 0.038*\"price\" + 0.028*\"steel\" + 0.027*\"cost\" + 0.022*\"product\" + 0.019*\"sell\" + 0.017*\"expect\" + 0.015*\"differ\" + 0.012*\"averag\" + 0.011*\"quarter\" + 0.009*\"market\"\n",
      "Topic 4 : 0.020*\"market\" + 0.017*\"go\" + 0.014*\"thank\" + 0.013*\"quarter\" + 0.013*\"strong\" + 0.013*\"product\" + 0.012*\"project\" + 0.011*\"busi\" + 0.011*\"believ\" + 0.010*\"good\"\n",
      "Topic 5 : 0.034*\"product\" + 0.022*\"quarter\" + 0.019*\"expect\" + 0.016*\"market\" + 0.016*\"cost\" + 0.015*\"time\" + 0.014*\"steel\" + 0.013*\"energi\" + 0.013*\"tubular\" + 0.012*\"activ\"\n",
      "Topic 6 : 0.032*\"million\" + 0.022*\"product\" + 0.018*\"year\" + 0.018*\"quarter\" + 0.017*\"revenu\" + 0.016*\"water\" + 0.015*\"time\" + 0.013*\"sale\" + 0.013*\"increas\" + 0.012*\"result\"\n",
      "Topic 7 : 0.021*\"product\" + 0.017*\"tubular\" + 0.017*\"increas\" + 0.017*\"seen\" + 0.014*\"think\" + 0.013*\"result\" + 0.012*\"cost\" + 0.012*\"steel\" + 0.012*\"look\" + 0.012*\"signific\"\n",
      "Topic 8 : 0.035*\"quarter\" + 0.031*\"year\" + 0.028*\"million\" + 0.028*\"expect\" + 0.021*\"look\" + 0.020*\"think\" + 0.016*\"cost\" + 0.015*\"project\" + 0.013*\"record\" + 0.011*\"backlog\"\n",
      "Topic 9 : 0.060*\"quarter\" + 0.044*\"million\" + 0.029*\"year\" + 0.015*\"busi\" + 0.014*\"expect\" + 0.014*\"product\" + 0.013*\"market\" + 0.013*\"share\" + 0.012*\"sale\" + 0.012*\"result\"\n"
     ]
    }
   ],
   "source": [
    "for topic in lda_10.show_topics():\n",
    "    print(\"Topic\", topic[0], \":\", topic[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3b554e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity:  -6.879600814846877\n"
     ]
    }
   ],
   "source": [
    "print('Perplexity: ', lda_10.log_perplexity(bow_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6da3afd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coherence Score:  -6.402824932960739\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import CoherenceModel\n",
    "coherence_model_lda = CoherenceModel(model=lda_10, texts=call_data['clean_text'], dictionary=dictionary, coherence='u_mass')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43b912aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = []\n",
    "with open('data/stoplist.txt', 'r') as f:\n",
    "    stopwords = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2d843ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    \"\"\" preprocess string and remove words from custom stopword list. \"\"\"\n",
    "    result = []\n",
    "\n",
    "    for word in preprocess_string(text):\n",
    "        if word not in stopwords:\n",
    "            result.append(word)\n",
    "    return result\n",
    "\n",
    "call_data['clean_newstop'] = call_data['Sentence'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d78a7f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(1001 unique tokens: ['diann', 'announc', 'confer', 'earn', 'northwest']...)\n"
     ]
    }
   ],
   "source": [
    "new_dictionary = corpora.Dictionary(call_data['clean_newstop'])\n",
    "print(new_dictionary)\n",
    "\n",
    "new_corpus = [new_dictionary.doc2bow(text) for text in call_data['clean_newstop']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a912c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0 : 0.040*\"steel\" + 0.028*\"compar\" + 0.021*\"tubular\" + 0.021*\"group\" + 0.014*\"financi\" + 0.013*\"signific\" + 0.012*\"certainli\" + 0.011*\"water\" + 0.011*\"pipe\" + 0.011*\"statement\"\n",
      "Topic 1 : 0.019*\"steel\" + 0.017*\"signific\" + 0.016*\"water\" + 0.015*\"month\" + 0.013*\"believ\" + 0.012*\"opportun\" + 0.010*\"issu\" + 0.009*\"continu\" + 0.009*\"ago\" + 0.009*\"transmiss\"\n",
      "Topic 2 : 0.025*\"share\" + 0.021*\"project\" + 0.018*\"earn\" + 0.014*\"result\" + 0.012*\"activ\" + 0.010*\"pipe\" + 0.010*\"probabl\" + 0.010*\"ahead\" + 0.010*\"strong\" + 0.010*\"compar\"\n",
      "Topic 3 : 0.021*\"project\" + 0.018*\"gener\" + 0.016*\"result\" + 0.015*\"improv\" + 0.013*\"materi\" + 0.013*\"volum\" + 0.012*\"steel\" + 0.012*\"activ\" + 0.011*\"energi\" + 0.011*\"higher\"\n",
      "Topic 4 : 0.028*\"backlog\" + 0.019*\"month\" + 0.019*\"project\" + 0.015*\"book\" + 0.015*\"order\" + 0.013*\"result\" + 0.013*\"ahead\" + 0.011*\"certainli\" + 0.011*\"forecast\" + 0.011*\"report\"\n",
      "Topic 5 : 0.023*\"tubular\" + 0.017*\"strong\" + 0.015*\"steel\" + 0.015*\"result\" + 0.015*\"group\" + 0.014*\"bid\" + 0.013*\"water\" + 0.012*\"gener\" + 0.012*\"transmiss\" + 0.011*\"backlog\"\n",
      "Topic 6 : 0.021*\"project\" + 0.021*\"brian\" + 0.015*\"stimulu\" + 0.014*\"bid\" + 0.012*\"steel\" + 0.012*\"order\" + 0.011*\"improv\" + 0.011*\"pipe\" + 0.011*\"gross\" + 0.011*\"profit\"\n",
      "Topic 7 : 0.023*\"water\" + 0.020*\"backlog\" + 0.017*\"compar\" + 0.016*\"tubular\" + 0.014*\"strong\" + 0.014*\"transmiss\" + 0.013*\"net\" + 0.013*\"gross\" + 0.011*\"discuss\" + 0.011*\"continu\"\n",
      "Topic 8 : 0.029*\"water\" + 0.020*\"steel\" + 0.018*\"differ\" + 0.018*\"transmiss\" + 0.015*\"tax\" + 0.011*\"project\" + 0.011*\"approxim\" + 0.010*\"effect\" + 0.010*\"capit\" + 0.009*\"averag\"\n",
      "Topic 9 : 0.042*\"project\" + 0.023*\"certainli\" + 0.019*\"steel\" + 0.018*\"continu\" + 0.013*\"fund\" + 0.012*\"water\" + 0.011*\"agenc\" + 0.010*\"gener\" + 0.010*\"current\" + 0.009*\"opportun\"\n"
     ]
    }
   ],
   "source": [
    "lda_new = models.LdaModel(new_corpus, num_topics=10, id2word=new_dictionary)\n",
    "\n",
    "for topic in lda_new.show_topics():\n",
    "    print(\"Topic\", topic[0], \":\", topic[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc4dc933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.5499266), (1, 0.05000604), (2, 0.05000604), (3, 0.05000604), (4, 0.05000604), (5, 0.05000604), (6, 0.05000604), (7, 0.05000604), (8, 0.05000604), (9, 0.050025117)]\n",
      "[(0, 0.014287135), (1, 0.014286572), (2, 0.8714208), (3, 0.014286144), (4, 0.014286419), (5, 0.014286053), (6, 0.014287521), (7, 0.014286855), (8, 0.014286233), (9, 0.014286276)]\n",
      "[(0, 0.93076396)]\n",
      "[(0, 0.89998513), (1, 0.011112107), (2, 0.011112673), (3, 0.011113541), (4, 0.011113584), (5, 0.011112443), (6, 0.0111116385), (7, 0.011112282), (8, 0.011114783), (9, 0.011111833)]\n",
      "[(3, 0.92498994)]\n",
      "[(0, 0.016672745), (1, 0.016667735), (2, 0.84997666), (3, 0.016670074), (4, 0.016670566), (5, 0.016668493), (6, 0.016668448), (7, 0.016668517), (8, 0.016668335), (9, 0.01666842)]\n",
      "[(0, 0.1), (1, 0.1), (2, 0.1), (3, 0.1), (4, 0.1), (5, 0.1), (6, 0.1), (7, 0.1), (8, 0.1), (9, 0.1)]\n",
      "[(0, 0.020002535), (1, 0.02000292), (2, 0.020005587), (3, 0.020004375), (4, 0.020005584), (5, 0.8199632), (6, 0.020002196), (7, 0.02000614), (8, 0.020002533), (9, 0.020004896)]\n",
      "[(0, 0.020010464), (1, 0.020002734), (2, 0.020002205), (3, 0.02000322), (4, 0.81996214), (5, 0.020006903), (6, 0.02000185), (7, 0.020005394), (8, 0.02000424), (9, 0.020000843)]\n"
     ]
    }
   ],
   "source": [
    "for doc in new_corpus[0:9]:\n",
    "    print(lda_new.get_document_topics(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "85356b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity:  -7.250209451537484\n"
     ]
    }
   ],
   "source": [
    "print('Perplexity: ', lda_new.log_perplexity(new_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8de35568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity:  -7.250243200901119\n"
     ]
    }
   ],
   "source": [
    "print('Perplexity: ', lda_new.log_perplexity(new_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ba6ab997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coherence Score:  -18.18188088238054\n"
     ]
    }
   ],
   "source": [
    "coherence_model_lda = CoherenceModel(model=lda_new, texts=call_data['clean_text'], dictionary=dictionary, coherence='u_mass')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8db77700",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.matutils import corpus2csc\n",
    "all_topics = lda_new.get_document_topics(new_corpus, minimum_probability=0.0)\n",
    "all_topics_csr = corpus2csc(all_topics)\n",
    "all_topics_numpy = all_topics_csr.T.toarray()\n",
    "all_topics_df = pd.DataFrame(all_topics_numpy)\n",
    "\n",
    "classification_df = pd.concat([call_data, all_topics_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9b0199fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>PRES</th>\n",
       "      <th>TURN_AT_TALK</th>\n",
       "      <th>CEO</th>\n",
       "      <th>WORDCOUNT</th>\n",
       "      <th>Restatement Topic</th>\n",
       "      <th>FRAUD</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1114.000000</td>\n",
       "      <td>1114.000000</td>\n",
       "      <td>1114.000000</td>\n",
       "      <td>1114.000000</td>\n",
       "      <td>1114.000000</td>\n",
       "      <td>1114.000000</td>\n",
       "      <td>1114.000000</td>\n",
       "      <td>1114.000000</td>\n",
       "      <td>1114.000000</td>\n",
       "      <td>1114.000000</td>\n",
       "      <td>1114.000000</td>\n",
       "      <td>1114.000000</td>\n",
       "      <td>1114.000000</td>\n",
       "      <td>1114.000000</td>\n",
       "      <td>1114.000000</td>\n",
       "      <td>1114.000000</td>\n",
       "      <td>1114.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>556.500000</td>\n",
       "      <td>0.581688</td>\n",
       "      <td>15.447935</td>\n",
       "      <td>0.793537</td>\n",
       "      <td>18.205566</td>\n",
       "      <td>0.176840</td>\n",
       "      <td>0.352783</td>\n",
       "      <td>0.106894</td>\n",
       "      <td>0.100772</td>\n",
       "      <td>0.100839</td>\n",
       "      <td>0.114237</td>\n",
       "      <td>0.090887</td>\n",
       "      <td>0.099371</td>\n",
       "      <td>0.086434</td>\n",
       "      <td>0.102081</td>\n",
       "      <td>0.095157</td>\n",
       "      <td>0.103329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>321.728405</td>\n",
       "      <td>0.493504</td>\n",
       "      <td>19.025057</td>\n",
       "      <td>0.404949</td>\n",
       "      <td>8.940149</td>\n",
       "      <td>0.381705</td>\n",
       "      <td>0.636108</td>\n",
       "      <td>0.233505</td>\n",
       "      <td>0.223849</td>\n",
       "      <td>0.229745</td>\n",
       "      <td>0.244002</td>\n",
       "      <td>0.208439</td>\n",
       "      <td>0.227154</td>\n",
       "      <td>0.204559</td>\n",
       "      <td>0.231783</td>\n",
       "      <td>0.215157</td>\n",
       "      <td>0.232446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005264</td>\n",
       "      <td>0.005557</td>\n",
       "      <td>0.005265</td>\n",
       "      <td>0.005264</td>\n",
       "      <td>0.005264</td>\n",
       "      <td>0.005264</td>\n",
       "      <td>0.005264</td>\n",
       "      <td>0.005264</td>\n",
       "      <td>0.005264</td>\n",
       "      <td>0.005264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>278.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014288</td>\n",
       "      <td>0.014288</td>\n",
       "      <td>0.014288</td>\n",
       "      <td>0.014288</td>\n",
       "      <td>0.014287</td>\n",
       "      <td>0.014288</td>\n",
       "      <td>0.014287</td>\n",
       "      <td>0.014288</td>\n",
       "      <td>0.014287</td>\n",
       "      <td>0.014288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>556.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020004</td>\n",
       "      <td>0.020006</td>\n",
       "      <td>0.020005</td>\n",
       "      <td>0.020005</td>\n",
       "      <td>0.020003</td>\n",
       "      <td>0.020005</td>\n",
       "      <td>0.020004</td>\n",
       "      <td>0.020005</td>\n",
       "      <td>0.020004</td>\n",
       "      <td>0.020005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>834.750000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.033347</td>\n",
       "      <td>0.033356</td>\n",
       "      <td>0.033345</td>\n",
       "      <td>0.050001</td>\n",
       "      <td>0.033342</td>\n",
       "      <td>0.033344</td>\n",
       "      <td>0.033339</td>\n",
       "      <td>0.033349</td>\n",
       "      <td>0.033343</td>\n",
       "      <td>0.033347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1113.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.939994</td>\n",
       "      <td>0.952623</td>\n",
       "      <td>0.943732</td>\n",
       "      <td>0.939987</td>\n",
       "      <td>0.943736</td>\n",
       "      <td>0.939990</td>\n",
       "      <td>0.943740</td>\n",
       "      <td>0.935705</td>\n",
       "      <td>0.939984</td>\n",
       "      <td>0.949985</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0         PRES  TURN_AT_TALK          CEO    WORDCOUNT  \\\n",
       "count  1114.000000  1114.000000   1114.000000  1114.000000  1114.000000   \n",
       "mean    556.500000     0.581688     15.447935     0.793537    18.205566   \n",
       "std     321.728405     0.493504     19.025057     0.404949     8.940149   \n",
       "min       0.000000     0.000000      2.000000     0.000000     1.000000   \n",
       "25%     278.250000     0.000000      3.000000     1.000000    12.000000   \n",
       "50%     556.500000     1.000000      4.000000     1.000000    17.000000   \n",
       "75%     834.750000     1.000000     24.000000     1.000000    23.000000   \n",
       "max    1113.000000     1.000000     93.000000     1.000000    62.000000   \n",
       "\n",
       "       Restatement Topic        FRAUD            0            1            2  \\\n",
       "count        1114.000000  1114.000000  1114.000000  1114.000000  1114.000000   \n",
       "mean            0.176840     0.352783     0.106894     0.100772     0.100839   \n",
       "std             0.381705     0.636108     0.233505     0.223849     0.229745   \n",
       "min             0.000000     0.000000     0.005264     0.005557     0.005265   \n",
       "25%             0.000000     0.000000     0.014288     0.014288     0.014288   \n",
       "50%             0.000000     0.000000     0.020004     0.020006     0.020005   \n",
       "75%             0.000000     1.000000     0.033347     0.033356     0.033345   \n",
       "max             1.000000     2.000000     0.939994     0.952623     0.943732   \n",
       "\n",
       "                 3            4            5            6            7  \\\n",
       "count  1114.000000  1114.000000  1114.000000  1114.000000  1114.000000   \n",
       "mean      0.114237     0.090887     0.099371     0.086434     0.102081   \n",
       "std       0.244002     0.208439     0.227154     0.204559     0.231783   \n",
       "min       0.005264     0.005264     0.005264     0.005264     0.005264   \n",
       "25%       0.014288     0.014287     0.014288     0.014287     0.014288   \n",
       "50%       0.020005     0.020003     0.020005     0.020004     0.020005   \n",
       "75%       0.050001     0.033342     0.033344     0.033339     0.033349   \n",
       "max       0.939987     0.943736     0.939990     0.943740     0.935705   \n",
       "\n",
       "                 8            9  \n",
       "count  1114.000000  1114.000000  \n",
       "mean      0.095157     0.103329  \n",
       "std       0.215157     0.232446  \n",
       "min       0.005264     0.005264  \n",
       "25%       0.014287     0.014288  \n",
       "50%       0.020004     0.020005  \n",
       "75%       0.033343     0.033347  \n",
       "max       0.939984     0.949985  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3dbd7b08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fit_time': array([0.31362104, 0.32213783, 0.30219221, 0.31323647, 0.30397129]), 'score_time': array([0.04085565, 0.03294563, 0.03494191, 0.03191447, 0.03101945]), 'test_accuracy': array([0.82142857, 0.78571429, 0.79464286, 0.82142857, 0.82142857]), 'test_neg_log_loss': array([-0.71471155, -0.44971985, -0.75287328, -0.73258985, -0.45952129]), 'test_f1': array([0.33333333, 0.2       , 0.20689655, 0.33333333, 0.16666667]), 'test_roc_auc': array([0.68179348, 0.69809783, 0.66467391, 0.65380435, 0.63532609])}\n",
      "Mean Accuracy: 0.8089285714285713\n",
      "Mean F1: 0.24804597701149428\n",
      "Mean ROC: 0.6667391304347826\n",
      "Mean Log Loss: -0.621883163472536\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "n_splits = 5\n",
    "\n",
    "pred_vars = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9,]\n",
    "\n",
    "\n",
    "scoring = ['accuracy', 'neg_log_loss', 'f1', 'roc_auc']\n",
    "rf_base = RandomForestClassifier()\n",
    "cv_rf = cross_validate(rf_base, classification_df[pred_vars], classification_df['Restatement Topic'], cv=StratifiedShuffleSplit(n_splits), scoring=scoring)\n",
    "print(cv_rf)\n",
    "print(\"Mean Accuracy:\", cv_rf['test_accuracy'].mean())\n",
    "print(\"Mean F1:\", cv_rf['test_f1'].mean())\n",
    "print(\"Mean ROC:\", cv_rf['test_roc_auc'].mean())\n",
    "print(\"Mean Log Loss:\", cv_rf['test_neg_log_loss'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0433a8c",
   "metadata": {},
   "source": [
    "# Tasks\n",
    "\n",
    "1. Try fitting LDA with just 5 topics instead of 10. How does this affect human interpretability, perplexity, coherence, and classification performance?\n",
    "\n",
    "2. Try fitting LDA with 15 topics. How does this affect human interpretability, perplexity, coherence, and classification performance?\n",
    "\n",
    "3. In addition to Random Forest, try another classifier of your choosing. How does this compare to the Random Forest?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e5e145",
   "metadata": {},
   "source": [
    "# Optional Tasks\n",
    "\n",
    "1. Run sentiment analysis on this data. Does adding that to a classifier improve performance?\n",
    "\n",
    "2. See the section below on weighted word counts. Does using tf-idf improve human interpretability?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5275a36",
   "metadata": {},
   "source": [
    "# Task 1\n",
    "\n",
    "Try fitting LDA with just 5 topics instead of 10. How does this affect human interpretability, perplexity, coherence, and classification performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1d7e08bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0 : 0.020*\"group\" + 0.019*\"backlog\" + 0.017*\"water\" + 0.016*\"result\" + 0.016*\"lower\" + 0.015*\"transmiss\" + 0.014*\"pipe\" + 0.013*\"strong\" + 0.012*\"project\" + 0.012*\"steel\"\n",
      "Topic 1 : 0.026*\"steel\" + 0.014*\"certainli\" + 0.014*\"signific\" + 0.013*\"tubular\" + 0.013*\"result\" + 0.012*\"continu\" + 0.011*\"effect\" + 0.011*\"project\" + 0.010*\"energi\" + 0.010*\"statement\"\n",
      "Topic 2 : 0.018*\"continu\" + 0.017*\"compar\" + 0.016*\"believ\" + 0.015*\"gener\" + 0.013*\"half\" + 0.013*\"bid\" + 0.013*\"net\" + 0.013*\"incom\" + 0.012*\"activ\" + 0.011*\"share\"\n",
      "Topic 3 : 0.042*\"project\" + 0.015*\"tubular\" + 0.014*\"water\" + 0.014*\"higher\" + 0.012*\"steel\" + 0.012*\"compar\" + 0.011*\"materi\" + 0.011*\"discuss\" + 0.011*\"plan\" + 0.010*\"stimulu\"\n",
      "Topic 4 : 0.026*\"steel\" + 0.019*\"gener\" + 0.018*\"water\" + 0.014*\"record\" + 0.010*\"tubular\" + 0.009*\"inventori\" + 0.009*\"earn\" + 0.009*\"result\" + 0.009*\"sell\" + 0.008*\"posit\"\n"
     ]
    }
   ],
   "source": [
    "lda_new_2 = models.LdaModel(new_corpus, num_topics=5, id2word=new_dictionary)\n",
    "\n",
    "for topic in lda_new_2.show_topics():\n",
    "    print(\"Topic\", topic[0], \":\", topic[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7d1de343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.10002244), (1, 0.10002665), (2, 0.1000246), (3, 0.59989685), (4, 0.10002943)]\n",
      "[(0, 0.88521415), (1, 0.028718451), (2, 0.028634999), (3, 0.028606309), (4, 0.028826077)]\n",
      "[(0, 0.9380498), (1, 0.015489789), (2, 0.015464462), (3, 0.015550069), (4, 0.015445879)]\n",
      "[(0, 0.02239238), (1, 0.9105051), (2, 0.022325426), (3, 0.022495663), (4, 0.022281416)]\n",
      "[(0, 0.016834844), (1, 0.016838241), (2, 0.016740467), (3, 0.932851), (4, 0.016735407)]\n"
     ]
    }
   ],
   "source": [
    "for doc in new_corpus[0:5]:\n",
    "    print(lda_new_2.get_document_topics(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ddb0c74c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity:  -6.901986630853719\n",
      "Perplexity:  -6.901965556211364\n"
     ]
    }
   ],
   "source": [
    "print('Perplexity: ', lda_new_2.log_perplexity(new_corpus))\n",
    "print('Perplexity: ', lda_new_2.log_perplexity(new_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c12b8898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coherence Score:  -18.567963858613645\n"
     ]
    }
   ],
   "source": [
    "coherence_model_lda = CoherenceModel(model=lda_new_2, texts=call_data['clean_text'], dictionary=dictionary, coherence='u_mass')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "580bde32",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_topics = lda_new_2.get_document_topics(new_corpus, minimum_probability=0.0)\n",
    "all_topics_csr = corpus2csc(all_topics)\n",
    "all_topics_numpy = all_topics_csr.T.toarray()\n",
    "all_topics_df = pd.DataFrame(all_topics_numpy)\n",
    "\n",
    "classification_df = pd.concat([call_data, all_topics_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "efc1e50c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>PRES</th>\n",
       "      <th>TURN_AT_TALK</th>\n",
       "      <th>CEO</th>\n",
       "      <th>WORDCOUNT</th>\n",
       "      <th>Restatement Topic</th>\n",
       "      <th>FRAUD</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1114.000000</td>\n",
       "      <td>1114.000000</td>\n",
       "      <td>1114.000000</td>\n",
       "      <td>1114.000000</td>\n",
       "      <td>1114.000000</td>\n",
       "      <td>1114.000000</td>\n",
       "      <td>1114.000000</td>\n",
       "      <td>1114.000000</td>\n",
       "      <td>1114.000000</td>\n",
       "      <td>1114.000000</td>\n",
       "      <td>1114.000000</td>\n",
       "      <td>1114.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>556.500000</td>\n",
       "      <td>0.581688</td>\n",
       "      <td>15.447935</td>\n",
       "      <td>0.793537</td>\n",
       "      <td>18.205566</td>\n",
       "      <td>0.176840</td>\n",
       "      <td>0.352783</td>\n",
       "      <td>0.234218</td>\n",
       "      <td>0.183739</td>\n",
       "      <td>0.206125</td>\n",
       "      <td>0.212392</td>\n",
       "      <td>0.163526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>321.728405</td>\n",
       "      <td>0.493504</td>\n",
       "      <td>19.025057</td>\n",
       "      <td>0.404949</td>\n",
       "      <td>8.940149</td>\n",
       "      <td>0.381705</td>\n",
       "      <td>0.636108</td>\n",
       "      <td>0.319316</td>\n",
       "      <td>0.284007</td>\n",
       "      <td>0.301522</td>\n",
       "      <td>0.301806</td>\n",
       "      <td>0.266406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010623</td>\n",
       "      <td>0.010648</td>\n",
       "      <td>0.010635</td>\n",
       "      <td>0.010692</td>\n",
       "      <td>0.011438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>278.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033413</td>\n",
       "      <td>0.029147</td>\n",
       "      <td>0.029119</td>\n",
       "      <td>0.033370</td>\n",
       "      <td>0.028975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>556.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050957</td>\n",
       "      <td>0.050124</td>\n",
       "      <td>0.050479</td>\n",
       "      <td>0.050622</td>\n",
       "      <td>0.041776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>834.750000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.313860</td>\n",
       "      <td>0.102767</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.100170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1113.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.946339</td>\n",
       "      <td>0.948852</td>\n",
       "      <td>0.949544</td>\n",
       "      <td>0.954486</td>\n",
       "      <td>0.957246</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0         PRES  TURN_AT_TALK          CEO    WORDCOUNT  \\\n",
       "count  1114.000000  1114.000000   1114.000000  1114.000000  1114.000000   \n",
       "mean    556.500000     0.581688     15.447935     0.793537    18.205566   \n",
       "std     321.728405     0.493504     19.025057     0.404949     8.940149   \n",
       "min       0.000000     0.000000      2.000000     0.000000     1.000000   \n",
       "25%     278.250000     0.000000      3.000000     1.000000    12.000000   \n",
       "50%     556.500000     1.000000      4.000000     1.000000    17.000000   \n",
       "75%     834.750000     1.000000     24.000000     1.000000    23.000000   \n",
       "max    1113.000000     1.000000     93.000000     1.000000    62.000000   \n",
       "\n",
       "       Restatement Topic        FRAUD            0            1            2  \\\n",
       "count        1114.000000  1114.000000  1114.000000  1114.000000  1114.000000   \n",
       "mean            0.176840     0.352783     0.234218     0.183739     0.206125   \n",
       "std             0.381705     0.636108     0.319316     0.284007     0.301522   \n",
       "min             0.000000     0.000000     0.010623     0.010648     0.010635   \n",
       "25%             0.000000     0.000000     0.033413     0.029147     0.029119   \n",
       "50%             0.000000     0.000000     0.050957     0.050124     0.050479   \n",
       "75%             0.000000     1.000000     0.313860     0.102767     0.200000   \n",
       "max             1.000000     2.000000     0.946339     0.948852     0.949544   \n",
       "\n",
       "                 3            4  \n",
       "count  1114.000000  1114.000000  \n",
       "mean      0.212392     0.163526  \n",
       "std       0.301806     0.266406  \n",
       "min       0.010692     0.011438  \n",
       "25%       0.033370     0.028975  \n",
       "50%       0.050622     0.041776  \n",
       "75%       0.200000     0.100170  \n",
       "max       0.954486     0.957246  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7eb8fcad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fit_time': array([0.27995086, 0.27462554, 0.2818048 , 0.2818346 , 0.2579627 ]), 'score_time': array([0.03387499, 0.03191972, 0.0388999 , 0.03247809, 0.0319469 ]), 'test_accuracy': array([0.82142857, 0.78571429, 0.77678571, 0.78571429, 0.8125    ]), 'test_neg_log_loss': array([-0.45937906, -0.81721368, -1.09159659, -0.82516563, -0.83409488]), 'test_f1': array([0.375     , 0.2       , 0.32432432, 0.2       , 0.27586207]), 'test_roc_auc': array([0.6923913 , 0.60543478, 0.5923913 , 0.55407609, 0.56548913])}\n",
      "Mean Accuracy: 0.7964285714285715\n",
      "Mean F1: 0.27503727865796834\n",
      "Mean ROC: 0.6019565217391304\n",
      "Mean Log Loss: -0.805489966212023\n"
     ]
    }
   ],
   "source": [
    "n_splits = 5\n",
    "\n",
    "pred_vars = [0, 1, 2, 3, 4,]\n",
    "\n",
    "\n",
    "scoring = ['accuracy', 'neg_log_loss', 'f1', 'roc_auc']\n",
    "rf_base = RandomForestClassifier()\n",
    "cv_rf = cross_validate(rf_base, classification_df[pred_vars], classification_df['Restatement Topic'], cv=StratifiedShuffleSplit(n_splits), scoring=scoring)\n",
    "print(cv_rf)\n",
    "print(\"Mean Accuracy:\", cv_rf['test_accuracy'].mean())\n",
    "print(\"Mean F1:\", cv_rf['test_f1'].mean())\n",
    "print(\"Mean ROC:\", cv_rf['test_roc_auc'].mean())\n",
    "print(\"Mean Log Loss:\", cv_rf['test_neg_log_loss'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ca96d5",
   "metadata": {},
   "source": [
    "human interpretability: Topic 2 is the only cohearnt one that seems like they are talking about making a bid for a project and how much income they will make from it. The 5 topic human interpretability is worse than the original 10 topics in my opinon.\n",
    " \n",
    "perplexity: 5 topics: -6.908 vs 10 topics: -7.203\n",
    " \n",
    "coherence: 5 topics: -17.971 vs 10 topics: -18.127\n",
    " \n",
    "classification performance:\n",
    "5 Topics \\\n",
    "Mean Accuracy: 0.8107142857142857 \\\n",
    "Mean F1: 0.2939923628466454 \\\n",
    "Mean ROC: 0.6096739130434783 \\\n",
    "Mean Log Loss: -0.6767382105982549 \\\n",
    "VS. \\\n",
    "10 Topics \\\n",
    "Mean Accuracy: 0.8071428571428572 \\\n",
    "Mean F1: 0.2180214166421063 \\\n",
    "Mean ROC: 0.6490217391304347 \\\n",
    "Mean Log Loss: -0.5865378903125963 \n",
    "\n",
    "Accuracy and F1 improved while ROC and Log Loss did not improve.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b45986c",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "\n",
    "Try fitting LDA with 15 topics. How does this affect human interpretability, perplexity, coherence, and classification performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6b72c0ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0 : 0.029*\"project\" + 0.023*\"activ\" + 0.014*\"result\" + 0.014*\"group\" + 0.011*\"oper\" + 0.011*\"construct\" + 0.011*\"lower\" + 0.011*\"bid\" + 0.011*\"tubular\" + 0.009*\"fund\"\n",
      "Topic 1 : 0.019*\"tubular\" + 0.017*\"order\" + 0.014*\"countri\" + 0.014*\"project\" + 0.012*\"recent\" + 0.012*\"tax\" + 0.012*\"signific\" + 0.012*\"activ\" + 0.011*\"postpon\" + 0.011*\"pipe\"\n",
      "Topic 2 : 0.027*\"project\" + 0.026*\"steel\" + 0.019*\"believ\" + 0.011*\"fund\" + 0.011*\"plan\" + 0.011*\"ton\" + 0.011*\"effect\" + 0.010*\"gener\" + 0.008*\"acquir\" + 0.008*\"manufactur\"\n",
      "Topic 3 : 0.057*\"water\" + 0.047*\"transmiss\" + 0.039*\"steel\" + 0.028*\"group\" + 0.016*\"certainli\" + 0.011*\"gener\" + 0.010*\"rang\" + 0.010*\"result\" + 0.010*\"compar\" + 0.010*\"strong\"\n",
      "Topic 4 : 0.033*\"differ\" + 0.023*\"statement\" + 0.022*\"group\" + 0.017*\"certainli\" + 0.014*\"steel\" + 0.014*\"futur\" + 0.014*\"continu\" + 0.014*\"tubular\" + 0.012*\"profit\" + 0.012*\"plan\"\n",
      "Topic 5 : 0.019*\"project\" + 0.017*\"signific\" + 0.017*\"certainli\" + 0.015*\"tubular\" + 0.013*\"probabl\" + 0.013*\"compar\" + 0.012*\"continu\" + 0.012*\"case\" + 0.012*\"improv\" + 0.012*\"pipe\"\n",
      "Topic 6 : 0.025*\"report\" + 0.019*\"compar\" + 0.016*\"steel\" + 0.016*\"tubular\" + 0.014*\"profit\" + 0.014*\"lower\" + 0.014*\"gross\" + 0.014*\"tax\" + 0.014*\"stimulu\" + 0.011*\"rel\"\n",
      "Topic 7 : 0.025*\"bid\" + 0.024*\"project\" + 0.024*\"water\" + 0.020*\"week\" + 0.019*\"continu\" + 0.015*\"sell\" + 0.014*\"steel\" + 0.013*\"half\" + 0.013*\"improv\" + 0.013*\"transmiss\"\n",
      "Topic 8 : 0.029*\"project\" + 0.020*\"steel\" + 0.018*\"gener\" + 0.018*\"result\" + 0.013*\"higher\" + 0.013*\"specif\" + 0.013*\"issu\" + 0.013*\"brian\" + 0.012*\"sell\" + 0.011*\"volum\"\n",
      "Topic 9 : 0.016*\"group\" + 0.016*\"strong\" + 0.016*\"differ\" + 0.016*\"compar\" + 0.012*\"reason\" + 0.012*\"structur\" + 0.012*\"share\" + 0.012*\"tubular\" + 0.012*\"slightli\" + 0.008*\"volum\"\n",
      "Topic 10 : 0.018*\"steel\" + 0.017*\"energi\" + 0.015*\"compar\" + 0.014*\"tubular\" + 0.013*\"inventori\" + 0.013*\"water\" + 0.012*\"demand\" + 0.010*\"financi\" + 0.010*\"believ\" + 0.010*\"month\"\n",
      "Topic 11 : 0.021*\"backlog\" + 0.020*\"energi\" + 0.016*\"fund\" + 0.016*\"order\" + 0.016*\"job\" + 0.016*\"continu\" + 0.014*\"activ\" + 0.013*\"probabl\" + 0.013*\"strong\" + 0.013*\"bond\"\n",
      "Topic 12 : 0.028*\"project\" + 0.026*\"continu\" + 0.026*\"backlog\" + 0.021*\"certainli\" + 0.016*\"strong\" + 0.015*\"water\" + 0.014*\"share\" + 0.012*\"half\" + 0.009*\"infrastructur\" + 0.009*\"schedul\"\n",
      "Topic 13 : 0.025*\"group\" + 0.022*\"result\" + 0.019*\"steel\" + 0.016*\"order\" + 0.016*\"adjust\" + 0.016*\"higher\" + 0.016*\"tubular\" + 0.013*\"ahead\" + 0.013*\"materi\" + 0.010*\"stephani\"\n",
      "Topic 14 : 0.030*\"steel\" + 0.022*\"record\" + 0.017*\"share\" + 0.017*\"result\" + 0.017*\"tubular\" + 0.015*\"signific\" + 0.014*\"project\" + 0.012*\"pipe\" + 0.012*\"gross\" + 0.011*\"profit\"\n"
     ]
    }
   ],
   "source": [
    "lda_new_3 = models.LdaModel(new_corpus, num_topics=15, id2word=new_dictionary)\n",
    "#print(len(lda_new_3.show_topics()))\n",
    "for topic in lda_new_3.show_topics(num_topics=15):\n",
    "    #print()\n",
    "    print(\"Topic\", topic[0], \":\", topic[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3da4896b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.033336002), (1, 0.033336185), (2, 0.033336002), (3, 0.033336002), (4, 0.033336002), (5, 0.53329575), (6, 0.033336002), (7, 0.033336002), (8, 0.033336002), (9, 0.033336002), (10, 0.033336002), (11, 0.033336002), (12, 0.033336002), (13, 0.033336002), (14, 0.033336002)]\n",
      "[(1, 0.8666633)]\n",
      "[(9, 0.92820364)]\n",
      "[(14, 0.896295)]\n",
      "[(14, 0.92222023)]\n",
      "[(0, 0.011111328), (1, 0.51886714), (2, 0.011111335), (3, 0.01111132), (4, 0.33668563), (5, 0.011111324), (6, 0.011111324), (7, 0.011111328), (8, 0.011111325), (9, 0.011111327), (10, 0.011111327), (11, 0.0111113265), (12, 0.011111331), (13, 0.011111325), (14, 0.011111332)]\n",
      "[(0, 0.06666667), (1, 0.06666667), (2, 0.06666667), (3, 0.06666667), (4, 0.06666667), (5, 0.06666667), (6, 0.06666667), (7, 0.06666667), (8, 0.06666667), (9, 0.06666667), (10, 0.06666667), (11, 0.06666667), (12, 0.06666667), (13, 0.06666667), (14, 0.06666667)]\n",
      "[(0, 0.8133301), (1, 0.013333587), (2, 0.013333562), (3, 0.013333556), (4, 0.0133335665), (5, 0.013333556), (6, 0.013333547), (7, 0.013333577), (8, 0.013333538), (9, 0.0133335395), (10, 0.013333575), (11, 0.013333552), (12, 0.013333581), (13, 0.013333564), (14, 0.013333564)]\n",
      "[(0, 0.013333632), (1, 0.013333624), (2, 0.013333648), (3, 0.013333647), (4, 0.01333363), (5, 0.013333637), (6, 0.01333363), (7, 0.013333634), (8, 0.013333648), (9, 0.013333636), (10, 0.8133291), (11, 0.013333641), (12, 0.013333642), (13, 0.013333626), (14, 0.013333654)]\n",
      "[(0, 0.011111428), (1, 0.011111417), (2, 0.011111425), (3, 0.011111418), (4, 0.84444016), (5, 0.0111114355), (6, 0.011111407), (7, 0.011111409), (8, 0.01111141), (9, 0.011111412), (10, 0.011111446), (11, 0.011111416), (12, 0.0111114085), (13, 0.011111428), (14, 0.011111417)]\n",
      "[(0, 0.013333912), (1, 0.013333905), (2, 0.013333938), (3, 0.81332505), (4, 0.013333916), (5, 0.013333915), (6, 0.013333892), (7, 0.0133339), (8, 0.0133339595), (9, 0.013333979), (10, 0.013333946), (11, 0.013333957), (12, 0.013333942), (13, 0.013333908), (14, 0.013333927)]\n",
      "[(0, 0.016666776), (1, 0.016666772), (2, 0.016666893), (3, 0.016666774), (4, 0.01666678), (5, 0.01666677), (6, 0.016666794), (7, 0.01666679), (8, 0.016666764), (9, 0.016666776), (10, 0.766665), (11, 0.016666768), (12, 0.016666794), (13, 0.016666777), (14, 0.016666783)]\n",
      "[(11, 0.88332677)]\n",
      "[(9, 0.86666036)]\n",
      "[(0, 0.013333538), (1, 0.01333353), (2, 0.013333555), (3, 0.013333533), (4, 0.013333537), (5, 0.01333353), (6, 0.013333545), (7, 0.013333569), (8, 0.01333353), (9, 0.013333533), (10, 0.5583055), (11, 0.01333353), (12, 0.01333355), (13, 0.26835847), (14, 0.013333535)]\n"
     ]
    }
   ],
   "source": [
    "for doc in new_corpus[0:15]:\n",
    "    print(lda_new_3.get_document_topics(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6fc8dba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity:  -7.487039290774965\n",
      "Perplexity:  -7.488385576863903\n"
     ]
    }
   ],
   "source": [
    "print('Perplexity: ', lda_new_3.log_perplexity(new_corpus))\n",
    "print('Perplexity: ', lda_new_3.log_perplexity(new_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2cf7a49b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coherence Score:  -18.33191010177371\n"
     ]
    }
   ],
   "source": [
    "coherence_model_lda = CoherenceModel(model=lda_new_3, texts=call_data['clean_text'], dictionary=dictionary, coherence='u_mass')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5d22afc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_topics = lda_new_3.get_document_topics(new_corpus, minimum_probability=0.0)\n",
    "all_topics_csr = corpus2csc(all_topics)\n",
    "all_topics_numpy = all_topics_csr.T.toarray()\n",
    "all_topics_df = pd.DataFrame(all_topics_numpy)\n",
    "\n",
    "classification_df = pd.concat([call_data, all_topics_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4860c64a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>PRES</th>\n",
       "      <th>TURN_AT_TALK</th>\n",
       "      <th>CEO</th>\n",
       "      <th>WORDCOUNT</th>\n",
       "      <th>Restatement Topic</th>\n",
       "      <th>FRAUD</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>...</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1114.000000</td>\n",
       "      <td>1114.000000</td>\n",
       "      <td>1114.000000</td>\n",
       "      <td>1114.000000</td>\n",
       "      <td>1114.000000</td>\n",
       "      <td>1114.000000</td>\n",
       "      <td>1114.000000</td>\n",
       "      <td>1114.000000</td>\n",
       "      <td>1114.000000</td>\n",
       "      <td>1114.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1114.000000</td>\n",
       "      <td>1114.000000</td>\n",
       "      <td>1114.000000</td>\n",
       "      <td>1114.000000</td>\n",
       "      <td>1114.000000</td>\n",
       "      <td>1114.000000</td>\n",
       "      <td>1114.000000</td>\n",
       "      <td>1114.000000</td>\n",
       "      <td>1114.000000</td>\n",
       "      <td>1114.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>556.500000</td>\n",
       "      <td>0.581688</td>\n",
       "      <td>15.447935</td>\n",
       "      <td>0.793537</td>\n",
       "      <td>18.205566</td>\n",
       "      <td>0.176840</td>\n",
       "      <td>0.352783</td>\n",
       "      <td>0.073247</td>\n",
       "      <td>0.067150</td>\n",
       "      <td>0.060742</td>\n",
       "      <td>...</td>\n",
       "      <td>0.051696</td>\n",
       "      <td>0.052546</td>\n",
       "      <td>0.072565</td>\n",
       "      <td>0.077235</td>\n",
       "      <td>0.071115</td>\n",
       "      <td>0.077117</td>\n",
       "      <td>0.074054</td>\n",
       "      <td>0.074296</td>\n",
       "      <td>0.065179</td>\n",
       "      <td>0.060940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>321.728405</td>\n",
       "      <td>0.493504</td>\n",
       "      <td>19.025057</td>\n",
       "      <td>0.404949</td>\n",
       "      <td>8.940149</td>\n",
       "      <td>0.381705</td>\n",
       "      <td>0.636108</td>\n",
       "      <td>0.197859</td>\n",
       "      <td>0.183461</td>\n",
       "      <td>0.182007</td>\n",
       "      <td>...</td>\n",
       "      <td>0.158835</td>\n",
       "      <td>0.162186</td>\n",
       "      <td>0.197851</td>\n",
       "      <td>0.204955</td>\n",
       "      <td>0.191248</td>\n",
       "      <td>0.199211</td>\n",
       "      <td>0.199990</td>\n",
       "      <td>0.200914</td>\n",
       "      <td>0.185033</td>\n",
       "      <td>0.182471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003509</td>\n",
       "      <td>0.003509</td>\n",
       "      <td>0.003509</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003509</td>\n",
       "      <td>0.003509</td>\n",
       "      <td>0.003509</td>\n",
       "      <td>0.003509</td>\n",
       "      <td>0.003509</td>\n",
       "      <td>0.003509</td>\n",
       "      <td>0.003509</td>\n",
       "      <td>0.003509</td>\n",
       "      <td>0.003509</td>\n",
       "      <td>0.003509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>278.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009524</td>\n",
       "      <td>0.009524</td>\n",
       "      <td>0.009524</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009524</td>\n",
       "      <td>0.009524</td>\n",
       "      <td>0.009524</td>\n",
       "      <td>0.009524</td>\n",
       "      <td>0.009524</td>\n",
       "      <td>0.009524</td>\n",
       "      <td>0.009524</td>\n",
       "      <td>0.009524</td>\n",
       "      <td>0.009524</td>\n",
       "      <td>0.009524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>556.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013334</td>\n",
       "      <td>0.013334</td>\n",
       "      <td>0.013334</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013334</td>\n",
       "      <td>0.013334</td>\n",
       "      <td>0.013334</td>\n",
       "      <td>0.013334</td>\n",
       "      <td>0.013334</td>\n",
       "      <td>0.013334</td>\n",
       "      <td>0.013334</td>\n",
       "      <td>0.013334</td>\n",
       "      <td>0.013334</td>\n",
       "      <td>0.013334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>834.750000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.022223</td>\n",
       "      <td>0.022223</td>\n",
       "      <td>0.022223</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022223</td>\n",
       "      <td>0.022223</td>\n",
       "      <td>0.022223</td>\n",
       "      <td>0.022223</td>\n",
       "      <td>0.022223</td>\n",
       "      <td>0.022224</td>\n",
       "      <td>0.022223</td>\n",
       "      <td>0.022223</td>\n",
       "      <td>0.022223</td>\n",
       "      <td>0.022223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1113.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.941664</td>\n",
       "      <td>0.915150</td>\n",
       "      <td>0.937775</td>\n",
       "      <td>...</td>\n",
       "      <td>0.941663</td>\n",
       "      <td>0.937775</td>\n",
       "      <td>0.933330</td>\n",
       "      <td>0.933330</td>\n",
       "      <td>0.950874</td>\n",
       "      <td>0.941664</td>\n",
       "      <td>0.937776</td>\n",
       "      <td>0.941664</td>\n",
       "      <td>0.950874</td>\n",
       "      <td>0.948146</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0         PRES  TURN_AT_TALK          CEO    WORDCOUNT  \\\n",
       "count  1114.000000  1114.000000   1114.000000  1114.000000  1114.000000   \n",
       "mean    556.500000     0.581688     15.447935     0.793537    18.205566   \n",
       "std     321.728405     0.493504     19.025057     0.404949     8.940149   \n",
       "min       0.000000     0.000000      2.000000     0.000000     1.000000   \n",
       "25%     278.250000     0.000000      3.000000     1.000000    12.000000   \n",
       "50%     556.500000     1.000000      4.000000     1.000000    17.000000   \n",
       "75%     834.750000     1.000000     24.000000     1.000000    23.000000   \n",
       "max    1113.000000     1.000000     93.000000     1.000000    62.000000   \n",
       "\n",
       "       Restatement Topic        FRAUD            0            1            2  \\\n",
       "count        1114.000000  1114.000000  1114.000000  1114.000000  1114.000000   \n",
       "mean            0.176840     0.352783     0.073247     0.067150     0.060742   \n",
       "std             0.381705     0.636108     0.197859     0.183461     0.182007   \n",
       "min             0.000000     0.000000     0.003509     0.003509     0.003509   \n",
       "25%             0.000000     0.000000     0.009524     0.009524     0.009524   \n",
       "50%             0.000000     0.000000     0.013334     0.013334     0.013334   \n",
       "75%             0.000000     1.000000     0.022223     0.022223     0.022223   \n",
       "max             1.000000     2.000000     0.941664     0.915150     0.937775   \n",
       "\n",
       "       ...            5            6            7            8            9  \\\n",
       "count  ...  1114.000000  1114.000000  1114.000000  1114.000000  1114.000000   \n",
       "mean   ...     0.051696     0.052546     0.072565     0.077235     0.071115   \n",
       "std    ...     0.158835     0.162186     0.197851     0.204955     0.191248   \n",
       "min    ...     0.003509     0.003509     0.003509     0.003509     0.003509   \n",
       "25%    ...     0.009524     0.009524     0.009524     0.009524     0.009524   \n",
       "50%    ...     0.013334     0.013334     0.013334     0.013334     0.013334   \n",
       "75%    ...     0.022223     0.022223     0.022223     0.022223     0.022223   \n",
       "max    ...     0.941663     0.937775     0.933330     0.933330     0.950874   \n",
       "\n",
       "                10           11           12           13           14  \n",
       "count  1114.000000  1114.000000  1114.000000  1114.000000  1114.000000  \n",
       "mean      0.077117     0.074054     0.074296     0.065179     0.060940  \n",
       "std       0.199211     0.199990     0.200914     0.185033     0.182471  \n",
       "min       0.003509     0.003509     0.003509     0.003509     0.003509  \n",
       "25%       0.009524     0.009524     0.009524     0.009524     0.009524  \n",
       "50%       0.013334     0.013334     0.013334     0.013334     0.013334  \n",
       "75%       0.022224     0.022223     0.022223     0.022223     0.022223  \n",
       "max       0.941664     0.937776     0.941664     0.950874     0.948146  \n",
       "\n",
       "[8 rows x 22 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0bdce58d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fit_time': array([0.31615424, 0.31910419, 0.3211751 , 0.31615305, 0.3219769 ]), 'score_time': array([0.03291202, 0.03324795, 0.03387499, 0.03291273, 0.03691149]), 'test_accuracy': array([0.8125    , 0.76785714, 0.77678571, 0.82142857, 0.75892857]), 'test_neg_log_loss': array([-0.5512609 , -0.63940361, -0.62787623, -0.44254006, -0.65758884]), 'test_f1': array([0.4       , 0.27777778, 0.3902439 , 0.41176471, 0.27027027]), 'test_roc_auc': array([0.66413043, 0.56576087, 0.60842391, 0.80543478, 0.60326087])}\n",
      "Mean Accuracy: 0.7875\n",
      "Mean F1: 0.35001133127388506\n",
      "Mean ROC: 0.6494021739130434\n",
      "Mean Log Loss: -0.5837339276642147\n"
     ]
    }
   ],
   "source": [
    "n_splits = 5\n",
    "\n",
    "pred_vars = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14,]\n",
    "\n",
    "\n",
    "scoring = ['accuracy', 'neg_log_loss', 'f1', 'roc_auc']\n",
    "rf_base = RandomForestClassifier()\n",
    "cv_rf = cross_validate(rf_base, classification_df[pred_vars], classification_df['Restatement Topic'], cv=StratifiedShuffleSplit(n_splits), scoring=scoring)\n",
    "print(cv_rf)\n",
    "print(\"Mean Accuracy:\", cv_rf['test_accuracy'].mean())\n",
    "print(\"Mean F1:\", cv_rf['test_f1'].mean())\n",
    "print(\"Mean ROC:\", cv_rf['test_roc_auc'].mean())\n",
    "print(\"Mean Log Loss:\", cv_rf['test_neg_log_loss'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2857e7",
   "metadata": {},
   "source": [
    "human interpretability: The human interpretability is better thasn both the 10 and 5 topic models, it is much more readable than those models.\n",
    " \n",
    "perplexity: 15 topics: -6.908 vs 10 topics: -7.404\n",
    " \n",
    "coherence: 15 topics: -17.971 vs 10 topics: -18.987\n",
    " \n",
    "classification performance: \\\n",
    "15 Topics: \\\n",
    "Mean Accuracy: 0.7839285714285713 \\\n",
    "Mean F1: 0.257784917486056 \\\n",
    "Mean ROC: 0.6614130434782608 \\\n",
    "Mean Log Loss: -0.5294505446969614\n",
    "\n",
    "VS.\n",
    "\n",
    "5 Topics: \\\n",
    "Mean Accuracy: 0.8107142857142857 \\\n",
    "Mean F1: 0.2939923628466454 \\\n",
    "Mean ROC: 0.6096739130434783 \\\n",
    "Mean Log Loss: -0.6767382105982549\n",
    "\n",
    "VS.\n",
    "\n",
    "10 Topics: \\\n",
    "Mean Accuracy: 0.8071428571428572 \\\n",
    "Mean F1: 0.2180214166421063 \\\n",
    "Mean ROC: 0.6490217391304347 \\\n",
    "Mean Log Loss: -0.5865378903125963\n",
    "\n",
    "Accuracy and F1 got worse while ROC and Log Loss did improved from the 5 Topics and 15 Topics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a806f1",
   "metadata": {},
   "source": [
    "# Task 3\n",
    "\n",
    "In addition to Random Forest, try another classifier of your choosing. How does this compare to the Random Forest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "aa933e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fit_time': array([0.0947454 , 0.08436894, 0.08178115, 0.08676839, 0.08326721]), 'score_time': array([0.02194142, 0.02048635, 0.01994681, 0.01998401, 0.01894975]), 'test_accuracy': array([0.79464286, 0.82142857, 0.80357143, 0.76785714, 0.83928571]), 'test_neg_log_loss': array([-0.68552591, -0.68551032, -0.67388198, -0.67240269, -0.65578137]), 'test_f1': array([0.14814815, 0.23076923, 0.26666667, 0.13333333, 0.30769231]), 'test_roc_auc': array([0.60679348, 0.61005435, 0.73478261, 0.5375    , 0.64891304])}\n",
      "Mean Accuracy: 0.8053571428571427\n",
      "Mean F1: 0.21732193732193733\n",
      "Mean ROC: 0.6276086956521739\n",
      "Mean Log Loss: -0.6746204558253005\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC #Does not work why?\n",
    "from sklearn.neural_network import MLPClassifier #Does not work why? works now\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "n_splits = 5\n",
    "\n",
    "pred_vars = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14,]\n",
    "\n",
    "\n",
    "scoring = ['accuracy', 'neg_log_loss', 'f1', 'roc_auc']\n",
    "kn_base = AdaBoostClassifier()\n",
    "\n",
    "cv_kn = cross_validate(kn_base, classification_df[pred_vars], classification_df['Restatement Topic'], cv=StratifiedShuffleSplit(n_splits), scoring=scoring)\n",
    "print(cv_kn)\n",
    "print(\"Mean Accuracy:\", cv_kn['test_accuracy'].mean())\n",
    "print(\"Mean F1:\", cv_kn['test_f1'].mean())\n",
    "print(\"Mean ROC:\", cv_kn['test_roc_auc'].mean())\n",
    "print(\"Mean Log Loss:\", cv_kn['test_neg_log_loss'].mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339a3ff3",
   "metadata": {},
   "source": [
    "15 Topics Random Forest: \\\n",
    "Mean Accuracy: 0.7875 \\\n",
    "Mean F1: 0.35001133127388506 \\\n",
    "Mean ROC: 0.6494021739130434 \\\n",
    "Mean Log Loss: -0.5837339276642147"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778fd15c",
   "metadata": {},
   "source": [
    "As an alternative model I used an AdaBoost Classifier Model as alternative. This model had an higher accuracy but lower F1 and ROC scores and a worse Mean Log Loss. This would suggest that the AdaBoost Classifier has a worse False Positive rate than the Random Forrest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c2ab48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
